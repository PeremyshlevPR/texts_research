<h1 align = 'center'>Исследовательская работа по классификации текстовых данных</h1>

<hr>

<h2>Задание на исследование:</h2>
<ol>
  <li>Провести предварительную обработку бинарной выборки, содержащей документы, относящиеся к анализу данных и не относящиеся. Использовать модели представления текстовых данных: Bag of Words, Word2Vec, FastText.</li>
<li>Визуализировать выборку с помощью метода главных компонент и t-SNE.</li>
<li>Провести классификацию, используя методы: логистическая регрессия, к-ближайших соседей, случайный лес.</li>
<li>Проанализировать результаты по критериям: полнота-точность, F-мера (при необходимости использовать ROC-кривую). Выбрать наилучшее решение (модель представления данных + метод классификации). </li>
</ol>

<hr>
<h2>Используемые библиотеки:</h2>

<ul>
  <li>	<i>Numpy</i> </li>
  <li>	<i>Pandas</i></li>
  <li>	<i>Json</i> – для чтения данных из json файла</li>
  <li>	<i>Re</i> – была использована для удаления знаков пунктуации</li>
  <li>	<i>Os</i> </li>
  <li>	<i>Sklearn</i> – отсюда взяты классификаторы, PCA, TSNE, GridSearch, TF-IDF и основные метрики, используемые для оценивания качества классификации</li>
  <li>	<i>Nltk</i> – список стоп-слов, стеммер</li>
  <li>	<i>Pymorphy</i> - лемматизация</li>
  <li>	<i>Matplotlib</i> – визуализация данных, отображение ROC кривых.</li>
  <li>	<i>Genism</i> – отсюда взяты Word2Vec и FastText</li>
  <li>	<i>Joblib</i> – для сохранения обученных моделей в файл и загрузки их из файла</li>
</ul>

<hr>
<h2>На стадии предобработки данных были применены следующие операции:</h2>

<ol>
  <li>Словарь стоп-слов взят из библиотеки nltk, также я добавил туда слова: 'в', 'и', 'на', 'с', 'также', 'для', которые встречались довольно часто в текстах.</li>
  <li>Лемматизация – <i>MorphAnalyzer()</i></li>
  <li>Стемминг - <i>SnowballStemmer("russian")</i></li>
</ol>

<h3>Удаление редко встречающихся слов</h3>
<div>
  Была применена функция FreqDist, которая проходилась по всем словам в названии статьи, ключевым словам и аннотациям к ним и считала количество слов. Таким образом самыми популярными словами оказались «анализ», «данные», «интеллектуальный».
</div>

<div>
  Всего слов во всех документах – 18009, при этом мы имеем 7350 слов, которые встречаются всего один раз и более 10000 слов, встречающихся два раза и менее.

Сначала попробовал обучить некоторые модели без удаления редко встречающихся слов, а затем после. Результат остался практически тем же. Но удалось существенно снизить размерность
</div>










