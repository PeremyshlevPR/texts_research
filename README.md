<h1 align = 'center'>Исследовательская работа по классификации текстовых данных</h1>

<hr>

<h2>Задание на исследование:</h2>
<ol>
  <li>Провести предварительную обработку бинарной выборки, содержащей документы, относящиеся к анализу данных и не относящиеся. Использовать модели представления текстовых данных: Bag of Words, Word2Vec, FastText.</li>
<li>Визуализировать выборку с помощью метода главных компонент и t-SNE.</li>
<li>Провести классификацию, используя методы: логистическая регрессия, к-ближайших соседей, случайный лес.</li>
<li>Проанализировать результаты по критериям: полнота-точность, F-мера (при необходимости использовать ROC-кривую). Выбрать наилучшее решение (модель представления данных + метод классификации). </li>
</ol>

<hr>
<h2>Используемые библиотеки:</h2>

<ul>
  <li>	<i>Numpy</i> </li>
  <li>	<i>Pandas</i></li>
  <li>	<i>Json</i> – для чтения данных из json файла</li>
  <li>	<i>Re</i> – была использована для удаления знаков пунктуации</li>
  <li>	<i>Os</i> </li>
  <li>	<i>Sklearn</i> – отсюда взяты классификаторы, PCA, TSNE, GridSearch, TF-IDF и основные метрики, используемые для оценивания качества классификации</li>
  <li>	<i>Nltk</i> – список стоп-слов, стеммер</li>
  <li>	<i>Pymorphy</i> - лемматизация</li>
  <li>	<i>Matplotlib</i> – визуализация данных, отображение ROC кривых.</li>
  <li>	<i>Genism</i> – отсюда взяты Word2Vec и FastText</li>
  <li>	<i>Joblib</i> – для сохранения обученных моделей в файл и загрузки их из файла</li>
</ul>

<hr>
<h2>На стадии предобработки данных были применены следующие операции:</h2>

<ol>
  <li>Словарь стоп-слов взят из библиотеки nltk, также я добавил туда слова: 'в', 'и', 'на', 'с', 'также', 'для', которые встречались довольно часто в текстах.</li>
  <li>Лемматизация – <i>MorphAnalyzer()</i></li>
  <li>Стемминг - <i>SnowballStemmer("russian")</i></li>
</ol>

<h3>Удаление редко встречающихся слов</h3>
<div>
  Была применена функция FreqDist, которая проходилась по всем словам в названии статьи, ключевым словам и аннотациям к ним и считала количество слов. Таким образом самыми популярными словами оказались «анализ», «данные», «интеллектуальный».
</div>

<div>
  Всего слов во всех документах – 18009, при этом мы имеем 7350 слов, которые встречаются всего один раз и более 10000 слов, встречающихся два раза и менее.

Сначала попробовал обучить некоторые модели без удаления редко встречающихся слов, а затем после. Результат остался практически тем же. Но удалось существенно снизить размерность
</div>

<hr>
<h2>Результаты визуализации</h2>
<div>Из результатов визуализации мы можем видеть, что классы довольно хорошо отделяются друг от друга. Особенно хорошо это видно на представлении t-SNE. PCA также неплохо отделяет данные друг от друга в случае с TF-IDF, в случае Word2Vec и FastText точки довольно сильно перемешиваются между собой. (Сами графики находятся в ноутбуке в соответствующем разделе) </div>

<hr>
<h2>Результаты классификации</h2>
<div> Результаты классификации занесены в таблицу </div>

<table border="1"><tr><td><p><b> </b></p>
</td>
<td colspan="3"><p>TF-IDF</p>
</td>
<td colspan="3"><p>Word2Vec</p>
</td>
<td colspan="3"><p>FastText</p>
</td>
</tr>
<tr><td><p><b> </b></p>
</td>
<td><p>Логистическая регрессия</p>
</td>
<td><p>KNN</p>
</td>
<td><p>Случайный лес</p>
</td>
<td><p>Логистическая регрессия</p>
</td>
<td><p>KNN</p>
</td>
<td><p>Случайный</p>
<p> лес</p>
</td>
<td><p>Логистическая регрессия</p>
</td>
<td><p>KNN</p>
</td>
<td><p>Случайный лес</p>
</td>
</tr>
<tr><td><p>Точность</p>
</td>
<td><p>0.978</p>
</td>
<td>0.729
</td>
<td>0.985
</td>
<td>0.965
</td>
<td>0.806
</td>
<td>0.910
</td>
<td>0.975
</td>
<td>0.807
</td>
<td>0.944
</td>
</tr>
<tr><td><p>Полнота</p>
</td>
<td>0.895
</td>
<td>0.927
</td>
<td>0.911</td>
<td>0.904
</td>
<td>0.927
</td>
<td>0.901
</td>
<td>0.901
</td>
<td>0.891
</td>
<td>0.891
</td>
</tr>
<tr><td><p>F1 мера</p>
</td>
<td>0.935</td>
<td>0.816
</td>
<td>0.947
</td>
<td>0.934
</td>
<td>0.862
</td>
<td>0.906
</td>
<td>0.936
</td>
<td>0.847
</td>
<td>0.917
</td>
</tr>
</table>

<div> Наилучшим образом себя показала модель TF-IDF + Random Forest. Она обладает наилучшими показателями по метрикам среди всех моделей. Также большим ее плюсом является то, что представление TF-IDF является наименее затратным среди всех прочих, как с точки зрения времени выполнения, так и с точки зрения экономии вычислительных ресурсов. Стоит отметить то, логистическая регрессия также показала себя очень хорошо. Показатели на всех моделях представления практически одинаковы, но опять же, стоит выбирать TF-IDF из-за его "простоты". </div>










